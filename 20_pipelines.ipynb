{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines in Python\n",
    "\n",
    "In Python, pipelines are commonly used in the context of machine learning and data preprocessing. They allow for the chaining together of multiple data transformations and a final estimator into a single workflow, which can be particularly useful in cross-validation for consistent and automated processing of data.\n",
    "\n",
    "In the scikit-learn library, pipelines can be created using the Pipeline class. Here's an example of creating a pipeline in scikit-learn:\n",
    "\n",
    "# Here are the key components of a pipelines :\n",
    "\n",
    "## 1.Data Preprocessing Steps: \n",
    "These are the initial steps in the pipeline that are used to transform and prepare the raw data for machine learning. Examples include data cleaning, data normalization, feature extraction, and feature engineering.\n",
    "## 2.Machine Learning Model:\n",
    " This is the final step in the pipeline, which uses the preprocessed data to make predictions or classifications. Common machine learning models include linear regression, logistic regression, decision trees, and neural networks.\n",
    "## 3.Estimator Class:\n",
    " In scikit-learn, pipelines are created using the Pipeline class, which is a type of estimator. The Pipeline class is used to chain together multiple data preprocessing steps and a final machine learning model into a single workflow.\n",
    "## 4.Fit Method:\n",
    " The fit method is used to train the machine learning model on the preprocessed data. In scikit-learn, this method is available on both individual estimators and pipelines.\n",
    "## 4.Predict Method:\n",
    " The predict method is used to make predictions on new data using the trained machine learning model. In scikit-learn, this method is available on both individual estimators and pipelines.\n",
    "## 5.Cross-Validation:\n",
    " Pipelines are particularly useful in cross-validation, as they ensure that the same preprocessing steps are applied to each fold of the data. This reduces variability and improves the accuracy of model evaluation. In scikit-learn, cross-validation can be performed using the cross_val_score function, which is available on both individual estimators and pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the titanic dataset from seaborn\n",
    "\n",
    "# df = sns.load_dataset('titanic')\n",
    "# df.head()\n",
    "\n",
    "# # select fratures and target variable\n",
    "# X = df[['pclass', 'age', 'sex','fare','embarked']]\n",
    "# y = df['survived']\n",
    "\n",
    "# # split the data into train and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # define the column transformer for imputing missing values and encoding categorical variables\n",
    "\n",
    "# numeric_features = ['age', 'fare']\n",
    "# categorical_features = ['pclass', 'sex', 'embarked']\n",
    "\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median'))\n",
    "# ])\n",
    "\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "# ])\n",
    "\n",
    "# preprocessor = ColumnTransformer(transformers=[\n",
    "#     ('num', numeric_transformer, numeric_features),\n",
    "#     ('cat', categorical_transformer, categorical_features)\n",
    "# ])\n",
    "\n",
    "# # create a pipeline with the preprocessor and RandomForestClassifier\n",
    "# pipeline = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', RandomForestClassifier())\n",
    "# ])\n",
    "\n",
    "# # fit the pipeline on the training data\n",
    "# pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # make predictions on the test data\n",
    "# y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# # calculate the accuracy score\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# # print out the confusion matrix\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# # print classification report \n",
    "\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "# Hyperperameter Tunning in Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "df = sns.load_dataset('titanic')\n",
    "df.head()\n",
    "\n",
    "# select fratures and target variable\n",
    "X = df[['pclass', 'age', 'sex','fare','embarked']]\n",
    "y = df['survived']\n",
    "# split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# crate a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer( strategy='most_frequent')), \n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "# define the hyperparameters to tune\n",
    "\n",
    "hyperparameters = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [None, 5, 10],\n",
    "    'model_min_samples_split': [2, 5, 10]\n",
    "}\n",
    "# create a grid search Cross-validation\n",
    "grid_search = GridSearchCV(pipeline,hyperparameters,cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# calculate accuracy score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy',accuracy)\n",
    "\n",
    "# print the best hyperparameters\n",
    "\n",
    "print('Best Hyperparameters:', grid_search.best_params_)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
